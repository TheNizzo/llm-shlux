{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining model\n",
    "Our model right now generates random tokens. We need to pretrain it on a corpus of text. Lets's take the model we built:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (output): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm()\n",
       "      (ln2): LayerNorm()\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm()\n",
       "  (head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from gpt_model.model import GPT\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "    \"context_length\": 256\n",
    "}\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = GPT(GPT_CONFIG)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, input_tensor, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    idx = input_tensor\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every efforte moves you IonATURES shortcomingsruction embarkedReward basemanodonselling credential\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_tokens(text, tokenizer):\n",
    "    encoding = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoding_tensor = torch.tensor(encoding).unsqueeze(0)\n",
    "    return encoding_tensor\n",
    "\n",
    "def tokens_to_text(tokens, tokenizer):\n",
    "    text = tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "    return text\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = \"Every efforte moves you\"\n",
    "token_ids = generate_text_simple(model, text_to_tokens(text, tokenizer), max_new_tokens=10, context_size=GPT_CONFIG[\"context_length\"])\n",
    "\n",
    "print(tokens_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to train the model on a corpus of text. We'll use the 1984 book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin n\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/1984.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  273973\n",
      "Total tokens:  66254\n"
     ]
    }
   ],
   "source": [
    "print(\"Total characters: \", len(text))\n",
    "total_tokens = len(tokenizer.encode(text))\n",
    "print(\"Total tokens: \", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text))\n",
    "train_data = text[:split_idx]\n",
    "val_data = text[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG[\"context_length\"],\n",
    "    stride=GPT_CONFIG[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG[\"context_length\"],\n",
    "    stride=GPT_CONFIG[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a loss function to train the model. We'll use the cross-entropy loss. It calculates the loss between the predicted probabilities and the true probabilities of the next token. Let's implement it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(input, target, model, device):\n",
    "    input_tensor = input.to(device)\n",
    "    target_tensor = target.to(device)\n",
    "    logits = model(input_tensor)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_tensor.flatten())\n",
    "    return loss\n",
    "\n",
    "def compute_loader_loss(dataloader, model, device, num_batches=None):\n",
    "    val_loss = 0.0\n",
    "    if len(dataloader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input, target) in enumerate(dataloader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "        loss = compute_loss(input, target, model, device)\n",
    "        val_loss += loss.item()\n",
    "    return val_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 10.9920, Validation loss: 10.9980\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = compute_loader_loss(train_loader, model, device)\n",
    "    val_loss = compute_loader_loss(val_loader, model, device)\n",
    "\n",
    "print(f\"Train loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to compute the loss on the training and validation sets. We can use this to monitor the performance of the model during training.\n",
    "\n",
    "We can now train the model.\n",
    "\n",
    "Let's implement the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = compute_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = compute_loader_loss(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = compute_loader_loss(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_tokens(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, input_tensor=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = tokens_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.947, Val loss 9.845\n",
      "Ep 1 (Step 000005): Train loss 8.313, Val loss 8.391\n",
      "Ep 1 (Step 000010): Train loss 7.472, Val loss 7.572\n",
      "Ep 1 (Step 000015): Train loss 7.261, Val loss 7.275\n",
      "Ep 1 (Step 000020): Train loss 7.110, Val loss 7.213\n",
      "Ep 1 (Step 000025): Train loss 7.049, Val loss 7.155\n",
      "Ep 1 (Step 000030): Train loss 6.819, Val loss 7.043\n",
      "Ep 1 (Step 000035): Train loss 6.766, Val loss 6.956\n",
      "Ep 1 (Step 000040): Train loss 6.785, Val loss 6.917\n",
      "Ep 1 (Step 000045): Train loss 6.609, Val loss 6.820\n",
      "Ep 1 (Step 000050): Train loss 6.621, Val loss 6.806\n",
      "Ep 1 (Step 000055): Train loss 6.655, Val loss 6.688\n",
      "Ep 1 (Step 000060): Train loss 6.556, Val loss 6.649\n",
      "Ep 1 (Step 000065): Train loss 6.453, Val loss 6.597\n",
      "Ep 1 (Step 000070): Train loss 6.382, Val loss 6.552\n",
      "Ep 1 (Step 000075): Train loss 6.280, Val loss 6.529\n",
      "Ep 1 (Step 000080): Train loss 6.139, Val loss 6.467\n",
      "Ep 1 (Step 000085): Train loss 6.124, Val loss 6.455\n",
      "Ep 1 (Step 000090): Train loss 6.017, Val loss 6.417\n",
      "Ep 1 (Step 000095): Train loss 5.975, Val loss 6.386\n",
      "Ep 1 (Step 000100): Train loss 6.040, Val loss 6.355\n",
      "Ep 1 (Step 000105): Train loss 5.783, Val loss 6.332\n",
      "Ep 1 (Step 000110): Train loss 5.995, Val loss 6.392\n",
      "Ep 1 (Step 000115): Train loss 6.119, Val loss 6.328\n",
      "Every effort moves you                             'ston was a                 \n",
      "Ep 2 (Step 000120): Train loss 5.885, Val loss 6.325\n",
      "Ep 2 (Step 000125): Train loss 5.775, Val loss 6.344\n",
      "Ep 2 (Step 000130): Train loss 5.860, Val loss 6.302\n",
      "Ep 2 (Step 000135): Train loss 5.921, Val loss 6.291\n",
      "Ep 2 (Step 000140): Train loss 5.811, Val loss 6.289\n",
      "Ep 2 (Step 000145): Train loss 5.795, Val loss 6.275\n",
      "Ep 2 (Step 000150): Train loss 5.766, Val loss 6.262\n",
      "Ep 2 (Step 000155): Train loss 5.794, Val loss 6.257\n",
      "Ep 2 (Step 000160): Train loss 5.812, Val loss 6.256\n",
      "Ep 2 (Step 000165): Train loss 5.644, Val loss 6.222\n",
      "Ep 2 (Step 000170): Train loss 5.881, Val loss 6.235\n",
      "Ep 2 (Step 000175): Train loss 5.538, Val loss 6.248\n",
      "Ep 2 (Step 000180): Train loss 5.801, Val loss 6.238\n",
      "Ep 2 (Step 000185): Train loss 5.622, Val loss 6.191\n",
      "Ep 2 (Step 000190): Train loss 5.593, Val loss 6.167\n",
      "Ep 2 (Step 000195): Train loss 5.531, Val loss 6.147\n",
      "Ep 2 (Step 000200): Train loss 5.473, Val loss 6.155\n",
      "Ep 2 (Step 000205): Train loss 5.649, Val loss 6.190\n",
      "Ep 2 (Step 000210): Train loss 5.280, Val loss 6.166\n",
      "Ep 2 (Step 000215): Train loss 5.414, Val loss 6.162\n",
      "Ep 2 (Step 000220): Train loss 5.534, Val loss 6.126\n",
      "Ep 2 (Step 000225): Train loss 5.499, Val loss 6.102\n",
      "Ep 2 (Step 000230): Train loss 5.385, Val loss 6.074\n",
      "Every effort moves you          'I to be a                      'I got. He had been a few. He had been\n",
      "Ep 3 (Step 000235): Train loss 5.243, Val loss 6.103\n",
      "Ep 3 (Step 000240): Train loss 5.486, Val loss 6.093\n",
      "Ep 3 (Step 000245): Train loss 5.210, Val loss 6.099\n",
      "Ep 3 (Step 000250): Train loss 5.097, Val loss 6.107\n",
      "Ep 3 (Step 000255): Train loss 5.315, Val loss 6.093\n",
      "Ep 3 (Step 000260): Train loss 5.367, Val loss 6.108\n",
      "Ep 3 (Step 000265): Train loss 5.240, Val loss 6.113\n",
      "Ep 3 (Step 000270): Train loss 5.434, Val loss 6.100\n",
      "Ep 3 (Step 000275): Train loss 5.231, Val loss 6.079\n",
      "Ep 3 (Step 000280): Train loss 5.428, Val loss 6.092\n",
      "Ep 3 (Step 000285): Train loss 5.175, Val loss 6.126\n",
      "Ep 3 (Step 000290): Train loss 5.052, Val loss 6.094\n",
      "Ep 3 (Step 000295): Train loss 5.325, Val loss 6.087\n",
      "Ep 3 (Step 000300): Train loss 5.134, Val loss 6.085\n",
      "Ep 3 (Step 000305): Train loss 5.089, Val loss 6.063\n",
      "Ep 3 (Step 000310): Train loss 5.212, Val loss 6.057\n",
      "Ep 3 (Step 000315): Train loss 5.100, Val loss 6.030\n",
      "Ep 3 (Step 000320): Train loss 5.051, Val loss 6.013\n",
      "Ep 3 (Step 000325): Train loss 4.993, Val loss 6.063\n",
      "Ep 3 (Step 000330): Train loss 5.145, Val loss 6.045\n",
      "Ep 3 (Step 000335): Train loss 5.307, Val loss 6.023\n",
      "Ep 3 (Step 000340): Train loss 4.970, Val loss 5.986\n",
      "Ep 3 (Step 000345): Train loss 4.948, Val loss 5.998\n",
      "Every effort moves you, and   'Yes, and the old man, and the same, and the old man, and the same way, and a sort of the  'Brien, and the same.   'Brien, and the same\n",
      "Ep 4 (Step 000350): Train loss 4.724, Val loss 6.021\n",
      "Ep 4 (Step 000355): Train loss 4.967, Val loss 6.010\n",
      "Ep 4 (Step 000360): Train loss 4.710, Val loss 5.994\n",
      "Ep 4 (Step 000365): Train loss 4.858, Val loss 6.063\n",
      "Ep 4 (Step 000370): Train loss 4.644, Val loss 6.037\n",
      "Ep 4 (Step 000375): Train loss 4.434, Val loss 6.053\n",
      "Ep 4 (Step 000380): Train loss 4.502, Val loss 6.059\n",
      "Ep 4 (Step 000385): Train loss 4.627, Val loss 6.028\n",
      "Ep 4 (Step 000390): Train loss 4.421, Val loss 6.022\n",
      "Ep 4 (Step 000395): Train loss 4.760, Val loss 6.000\n",
      "Ep 4 (Step 000400): Train loss 4.446, Val loss 6.018\n",
      "Ep 4 (Step 000405): Train loss 4.650, Val loss 6.006\n",
      "Ep 4 (Step 000410): Train loss 4.296, Val loss 6.010\n",
      "Ep 4 (Step 000415): Train loss 4.718, Val loss 6.006\n",
      "Ep 4 (Step 000420): Train loss 4.518, Val loss 6.034\n",
      "Ep 4 (Step 000425): Train loss 4.488, Val loss 6.042\n",
      "Ep 4 (Step 000430): Train loss 4.384, Val loss 6.002\n",
      "Ep 4 (Step 000435): Train loss 4.708, Val loss 6.007\n",
      "Ep 4 (Step 000440): Train loss 4.416, Val loss 5.996\n",
      "Ep 4 (Step 000445): Train loss 4.531, Val loss 6.024\n",
      "Ep 4 (Step 000450): Train loss 4.130, Val loss 6.019\n",
      "Ep 4 (Step 000455): Train loss 3.993, Val loss 6.023\n",
      "Ep 4 (Step 000460): Train loss 4.358, Val loss 6.029\n",
      "Every effort moves you  'I know you'd you  'I'm you'd you 'I'm you 'I'm you'd you'd you'd you'd you 'I'm you'd you'd you 'I'm you '\n",
      "Ep 5 (Step 000465): Train loss 4.155, Val loss 6.040\n",
      "Ep 5 (Step 000470): Train loss 4.258, Val loss 6.011\n",
      "Ep 5 (Step 000475): Train loss 4.111, Val loss 6.030\n",
      "Ep 5 (Step 000480): Train loss 3.990, Val loss 6.075\n",
      "Ep 5 (Step 000485): Train loss 4.033, Val loss 6.014\n",
      "Ep 5 (Step 000490): Train loss 4.239, Val loss 6.048\n",
      "Ep 5 (Step 000495): Train loss 4.259, Val loss 6.037\n",
      "Ep 5 (Step 000500): Train loss 3.768, Val loss 6.093\n",
      "Ep 5 (Step 000505): Train loss 3.930, Val loss 6.107\n",
      "Ep 5 (Step 000510): Train loss 3.493, Val loss 6.146\n",
      "Ep 5 (Step 000515): Train loss 4.030, Val loss 6.110\n",
      "Ep 5 (Step 000520): Train loss 3.867, Val loss 6.167\n",
      "Ep 5 (Step 000525): Train loss 4.054, Val loss 6.084\n",
      "Ep 5 (Step 000530): Train loss 3.878, Val loss 6.126\n",
      "Ep 5 (Step 000535): Train loss 3.468, Val loss 6.098\n",
      "Ep 5 (Step 000540): Train loss 3.757, Val loss 6.143\n",
      "Ep 5 (Step 000545): Train loss 3.636, Val loss 6.120\n",
      "Ep 5 (Step 000550): Train loss 3.980, Val loss 6.091\n",
      "Ep 5 (Step 000555): Train loss 3.607, Val loss 6.082\n",
      "Ep 5 (Step 000560): Train loss 3.299, Val loss 6.061\n",
      "Ep 5 (Step 000565): Train loss 3.422, Val loss 6.039\n",
      "Ep 5 (Step 000570): Train loss 3.315, Val loss 6.036\n",
      "Ep 5 (Step 000575): Train loss 3.348, Val loss 6.084\n",
      "Every effort moves you, and 'Brien, and the  'Brien.    'Brien's,       'Brien. 'The Times'Brien, and      'Brien. 'The Times\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model = GPT(GPT_CONFIG)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using temperature to control the randomness of the model. And top_k sampling to control the diversity of the model. We can now create a new generate function that uses temperature and top_k sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you,\n",
      "The street, with very fair of the past,\n",
      "into them\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model.to('cpu')\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_tokens(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", tokens_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the weights of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That way if we create another model with the same architecture, we can load the weights and continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6780/562406528.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"model.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you, I'd of the\n",
      "to the old. The patrols was too unusual\n"
     ]
    }
   ],
   "source": [
    "model = GPT(GPT_CONFIG)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_tokens(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4)\n",
    "\n",
    "print(\"Output text:\\n\", tokens_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Load from OpenAI\n",
    "Training on a small dataset is not enough to train a good model. And training on a large dataset requires a lot of computational resources. So we can use the weights that OpenAI trained and released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-22 14:41:54.601857: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-22 14:41:54.750581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-22 14:41:54.805124: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-22 14:41:54.984935: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-22 14:41:57.864774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 104kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.52MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 151kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:29<00:00, 16.6MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 15.0MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 829kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 793kiB/s] \n"
     ]
    }
   ],
   "source": [
    "# I use the script create by Sebastian Raschka to download the weights.\n",
    "from gpt_weights_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(settings)\n",
    "print(params.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can assign the weights to our model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
