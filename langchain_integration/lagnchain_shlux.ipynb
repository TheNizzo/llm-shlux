{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain integration\n",
    "In a world where the shlux LLM we built is good enough for use, langchain proves to be a very convenient tool for interacting with the model. Langchain has allowed us to easily create a chatbot that can interact with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-02 01:03:38.261896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-02 01:03:38.356587: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-02 01:03:38.385787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-02 01:03:38.534923: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-02 01:03:39.762616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.language_models.llms import LLM\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from typing import Any, List, Optional\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from gpt_model.model import GPT\n",
    "from pretraining_model.load_weights_into_gpt import load_weights_into_gpt\n",
    "from model_generate import text_to_tokens, generate, tokens_to_text\n",
    "from pretraining_model.gpt_weights_download import download_and_load_gpt2\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "\n",
    "class ShluxGPT(LLM):\n",
    "    \"\"\"\n",
    "    Custom LLM class.\n",
    "    \"\"\"\n",
    "    model: Any = None\n",
    "    tokenizer: Any = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = GPT({\n",
    "            \"vocab_size\": 50257,\n",
    "            \"n_embd\": 768,\n",
    "            \"n_heads\": 12,\n",
    "            \"n_layers\": 12,\n",
    "            \"dropout\": 0.1,\n",
    "            \"context_length\": 1024,\n",
    "            \"bias\": True\n",
    "        })\n",
    "        settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "        load_weights_into_gpt(self.model, params)\n",
    "        self.tokenizer = tiktoken.encoding_for_model(\"gpt2\")\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        token_ids = generate(\n",
    "            model=self.model,\n",
    "            idx=text_to_tokens(prompt, self.tokenizer),\n",
    "            max_new_tokens=15,\n",
    "            context_size=256,\n",
    "            top_k=25,\n",
    "            temperature=1.4,\n",
    "        )\n",
    "        return tokens_to_text(token_ids, self.tokenizer)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"shluxgpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 286kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.52MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 233kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:28<00:00, 17.2MiB/s] \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 160kiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 830kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 800kiB/s] \n",
      "2024-10-02 01:04:20.389298: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 154389504 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? [laughs]\n",
      "\n",
      "[laughing sound]\n",
      "\n",
      "And now you\n"
     ]
    }
   ],
   "source": [
    "llm = ShluxGPT(n=5)\n",
    "print(llm.invoke(\"Hello, how are you?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what do you know? I can't remember how it happened... I mean... I was watching someone\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"what do you know?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
