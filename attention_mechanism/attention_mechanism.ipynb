{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n",
    "attention from its name, is a mechanism that allows the model to focus on specific parts of the input sequence.\n",
    "Let's build a simple attention mechanism from scratch. Starting with the self-attention mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "# Let's compute the attention scores for each input.\n",
    "# We'll use the dot product of the query, key, and value.\n",
    "# The query, key, and value are the same in this case.\n",
    "# So we'll use the same linear layer to compute the query, key, and value.\n",
    "# The query, key, and value are the same in this case.\n",
    "# So we'll use the same linear layer to compute the query, key, and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3755, 0.2777],\n",
      "        [0.3761, 0.2831],\n",
      "        [0.3761, 0.2833],\n",
      "        [0.3768, 0.2763],\n",
      "        [0.3754, 0.2836],\n",
      "        [0.3772, 0.2746]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        queries = self.query(x)\n",
    "        keys = self.key(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        # Compute the attention scores\n",
    "        attention_scores = torch.matmul(queries, keys.T)\n",
    "        attention_weights= torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=1)\n",
    "        output = torch.matmul(attention_weights, values)\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "attention_output = SelfAttention(d_in, d_out)(inputs)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above is that the attention mechanism can look at all the tokens in the input sequence. In generation tasks, we want the model to look at the previous tokens.\n",
    "To achieve this, we can use a causal mask. Let's modify the SelfAttention class to include a causal mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Self-Attention (Masked Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4921, 0.1196],\n",
      "         [0.5174, 0.2886],\n",
      "         [0.5257, 0.3366],\n",
      "         [0.4595, 0.3246],\n",
      "         [0.4531, 0.2852],\n",
      "         [0.2807, 0.1521]],\n",
      "\n",
      "        [[0.4921, 0.1196],\n",
      "         [0.5174, 0.2886],\n",
      "         [0.5257, 0.3366],\n",
      "         [0.4595, 0.3246],\n",
      "         [0.3358, 0.1922],\n",
      "         [0.4191, 0.3051]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.value = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, num_tokens, _ = x.shape\n",
    "        keys = self.key(x)\n",
    "        queries = self.query(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        attn_scores = torch.matmul(queries, keys.transpose(1, 2))\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        output = torch.matmul(attn_weights, values)\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "attention_output = CausalSelfAttention(d_in, d_out, context_length=context_length)(batch)\n",
    "print(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have build what we call a single-head attention mechanism. We can do better by using multiple attention heads, which helps the model to focus on different parts of the input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1268, -0.1532],\n",
      "         [-0.1070, -0.1730],\n",
      "         [-0.1016, -0.1791],\n",
      "         [-0.0842, -0.1591],\n",
      "         [-0.0892, -0.1540],\n",
      "         [-0.0760, -0.1452]],\n",
      "\n",
      "        [[-0.1268, -0.1532],\n",
      "         [-0.1070, -0.1730],\n",
      "         [-0.1016, -0.1791],\n",
      "         [-0.0842, -0.1591],\n",
      "         [-0.0892, -0.1540],\n",
      "         [-0.0760, -0.1452]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.d_out = d_out\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.value = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.output = nn.Linear(d_out, d_out, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, _ = x.shape\n",
    "\n",
    "        keys = self.key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.query(x)\n",
    "        values = self.value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        output = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        output = output.contiguous().view(b, num_tokens, self.d_out)\n",
    "        output = self.output(output) # optional projection\n",
    "\n",
    "        return output\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 2, 0.0)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
