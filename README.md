# llm-shlux

Buildling blocks for preparing, developping and pretraining a GPT like LLM. Influenced from Sebastian Raschka's [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch).

This work is for my own learning on how LLMs work. It allowed to gain a deeper understanding of the main building blocks of a famous LLM such as the gpt-2 model created by OpenAI.

## Contents

1. Tokenization: [tokenizer.ipynb](tokenization/tokenizer.ipynb)
2. Attention Mechanism: [attention_mechanism.ipynb](attention_mechanism/attention_mechanism.ipynb)
3. GPT Model: [model.ipynb](gpt_model/model.ipynb)
4. Pretraining Model: [pretrain_model.ipynb](pretraining_model/pretrain_model.ipynb)

I'm planning on continuing the extension of the notebooks, to include finetuning the model, exploring differences between modern LLMs, and possibly Parameter Efficient Finetuning (PEF).
