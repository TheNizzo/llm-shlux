{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple tokenizer takes text and splits it into tokens. it mainly has an encoder and decoder.\n",
    "- The encoder turns text into token IDs.\n",
    "- The decoder turns token IDs back into text.\n",
    "\n",
    "Let's use the La La Land movie script to demonstrate tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA LA LAND\n",
      "by\n",
      "Damien Chazelle\n",
      "\n",
      "Total number of characters: 102179\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/la_la_land.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(raw_text[:30])\n",
    "print(\"Total number of characters:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to turn the raw text into a sequence of tokens usable by an LLM.\n",
    "First, we need to preprocess the text to remove unwanted characters and split it into tokens.\n",
    "I have chosen the following set of characters to be removed:\n",
    "- Punctuation: ,.:;?_!\"()'\n",
    "- Hyphens: -\n",
    "- Whitespace: \\s\n",
    "- Double hyphen: --\n",
    "Alot more characters can be removed, but for now, let's keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LA', 'LA', 'LAND', 'by', 'Damien', 'Chazelle', 'FADE', 'IN', '.', '.', '.', 'A', 'sun-blasted', 'sky', '.', 'We', 'HEAR', 'radios', '--', 'one', 'piece', 'of', 'music', 'after', 'another', '.', '.', '.', 'We’re', '--']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23341\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these tokens we can build a vocabulary of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3836\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(set(preprocessed))\n",
    "print(len(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: i for i, token in enumerate(all_tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PUSH', 1000),\n",
       " ('PUSHED', 1001),\n",
       " ('Pantages', 1002),\n",
       " ('Paris', 1003),\n",
       " ('Parisian', 1004),\n",
       " ('Parisian-style', 1005),\n",
       " ('Park', 1006),\n",
       " ('Parker', 1007),\n",
       " ('Pasadena', 1008),\n",
       " ('Passes', 1009),\n",
       " ('Passing', 1010),\n",
       " ('Pasta', 1011),\n",
       " ('Peer', 1012),\n",
       " ('Peers', 1013),\n",
       " ('People', 1014),\n",
       " ('Pfeiffer’s', 1015),\n",
       " ('Photographer', 1016),\n",
       " ('Photographer’s', 1017),\n",
       " ('Piano', 1018),\n",
       " ('Picks', 1019)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.items())[1000:1020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a vocabulary, we can encode the raw text into token IDs. We can build a simple tokenizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # split the text into tokens, by the same characters we built the vocab from.\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()] # remove leading and trailing whitespace\n",
    "        return [self.vocab[token] for token in preprocessed] # return the token IDs\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([{i:s for s,i in self.vocab.items()}[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[804, 679, 2768, 2752, 3525, 2867, 3727, 1431, 3675, 3545, 1913, 2434, 1664, 2713, 2266, 1381, 1707, 10]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TokenizerV1(vocab)\n",
    "text = \"\"\"Maybe I’m one of those people who’s\n",
    "always wanted to do it but never had a\n",
    "chance.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maybe I’m one of those people who’s always wanted to do it but never had a chance.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's use a sentence containing a word that is not in the vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Instagram'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mBut on Instagram it said you liked hip hop more than jazz!\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36mTokenizerV1.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \u001b[38;5;66;03m# split the text into tokens, by the same characters we built the vocab from.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()] \u001b[38;5;66;03m# remove leading and trailing whitespace\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[token] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text) \u001b[38;5;66;03m# split the text into tokens, by the same characters we built the vocab from.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()] \u001b[38;5;66;03m# remove leading and trailing whitespace\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Instagram'"
     ]
    }
   ],
   "source": [
    "text = \"\"\"But on Instagram it said you liked hip hop more than jazz!\"\"\"\n",
    "ids = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a problem, but its one of the reasons that special tokens are useful.\n",
    "\n",
    "Inspired by GPT-2, let's add special tokens to our vocab:\n",
    "- \"<|endoftext|>\" which is used in GPT-2 denoting the end of a text. \n",
    "- \"<|unk|>\" to denote unknown tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3838\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('“working”', 3833)\n",
      "('”', 3834)\n",
      "('”Casting”', 3835)\n",
      "('<|endoftext|>', 3836)\n",
      "('<|unk|>', 3837)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the tokenizer the include the special token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) # split the text into tokens, by the same characters we built the vocab from.\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()] # remove leading and trailing whitespace\n",
    "        preprocessed = [item if item in self.vocab else \"<|unk|>\" for item in preprocessed]\n",
    "        return [self.vocab[token] for token in preprocessed] # return the token IDs\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([{i:s for s,i in self.vocab.items()}[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to tokenize again, now using the new tokenzier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[248, 2765, 3837, 2434, 3110, 3778, 2539, 3837, 2354, 2662, 3492, 2444, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TokenizerV2(vocab)\n",
    "text = \"\"\"But on Instagram it said you liked hip hop more than jazz!\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But on <|unk|> it said you liked <|unk|> hop more than jazz!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE (Byte-Pair Encoding)\n",
    "BytePair encoding allows for unknown words in the text to be dissected to smaller parts sometimes even 1 character at a time depending on the trained BPE merges. \n",
    "Let's use OpenAi's opensource tiktoken library to showcase the encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import importlib\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1537, 319, 10767, 340, 531, 345, 8288, 10359, 1725, 517, 621, 21274, 0, 220, 50256, 1119, 531, 262, 1306, 3155, 286, 1528, 986, 887, 198, 40, 447, 247, 76, 407, 12451, 284, 1064, 1997, 503, 13]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"But on Instagram it said you liked hip hop more than jazz! <|endoftext|> They said the next couple of days... But\n",
    "I’m not expecting to find anything out.\"\"\"\n",
    "ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But on Instagram it said you liked hip hop more than jazz! <|endoftext|> They said the next couple of days... But\\nI’m not expecting to find anything out.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
